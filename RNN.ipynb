{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unno/git/cupy/cupy/core/fusion.py:659: FutureWarning: cupy.core.fusion is experimental. The interface can change in the future.\n",
      "  util.experimental('cupy.core.fusion')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CHAINER_TYPE_CHECK'] = '0'\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import Variable\n",
    "from chainer import cuda\n",
    "from common.params_lstm import *\n",
    "from common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer.config.type_check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.set_max_workspace_size(512 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.6.2 (default, Nov 13 2017, 16:19:19) \n",
      "[GCC 5.4.0 20160609]\n",
      "Chainer:  4.0.0b1\n",
      "CuPy:  4.0.0b1\n",
      "Numpy:  1.13.3\n",
      "GPU:  ['Quadro K420', 'GeForce GTX TITAN X', 'GeForce GTX TITAN X', 'GeForce GTX TITAN X', 'GeForce GTX TITAN X']\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Chainer: \", chainer.__version__)\n",
    "print(\"CuPy: \", chainer.cuda.cupy.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"GPU: \", get_gpu_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolModule(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units):\n",
    "        super(SymbolModule, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed = L.EmbedID(n_vocab, EMBEDSIZE)\n",
    "            self.gru = L.NStepGRU(1, EMBEDSIZE, n_units, 0)\n",
    "            self.l_out = L.Linear(n_units, 2)\n",
    "\n",
    "    def ___call__(self, x_data):\n",
    "        batchsize = len(x_data)\n",
    "        hx = None\n",
    "        xs = []\n",
    "        lengths = []\n",
    "        for i, x in enumerate(x_data):\n",
    "            x = Variable(x)\n",
    "            x = self.embed(x)\n",
    "            xs.append(x)\n",
    "            lengths.append(len(x))\n",
    "        # GRU\n",
    "        _hy, ys = self.gru(hx=hx, xs=xs)\n",
    "\n",
    "        last_idx = np.cumsum(lengths).astype(np.int32) - 1\n",
    "        last_idx = cuda.to_gpu(last_idx)\n",
    "\n",
    "        last_vecs = F.embed_id(last_idx, F.concat(ys, axis=0))\n",
    "        y = self.l_out(last_vecs)\n",
    "        return y\n",
    "\n",
    "    def __call__(self, x_data):\n",
    "        batchsize = len(x_data)\n",
    "        hx = None\n",
    "        x_data = F.transpose(x_data)\n",
    "        xs = self.embed(x_data)\n",
    "        #xs = F.split_axis(xs.reshape(-1, xs.shape[2]), batchsize, 0)\n",
    "        xs = F.split_axis(xs.reshape(-1, xs.shape[2]), xs.shape[0], 0)\n",
    "\n",
    "        # GRU\n",
    "        _hy, ys = self.gru(hx=hx, xs=xs)\n",
    "        ys = F.transpose_sequence(ys)\n",
    "        shape = (len(ys),) + ys[0].shape\n",
    "        last_vecs = F.concat(ys, axis=0).reshape(shape)[:, -1, :]\n",
    "        #shape = (ys[0].shape[0], len(ys), ys[0].shape[1])\n",
    "        #last_vecs = F.stack(ys).transpose([1, 0, 2]).reshape(shape)[:, -1, :]\n",
    "        y = self.l_out(last_vecs)\n",
    "        return y\n",
    "    \n",
    "    def __call__(self, x_data):\n",
    "        x_data = x_data.T\n",
    "        lengths = np.full(len(x_data), x_data.shape[1], dtype='i')\n",
    "        x_data = x_data.reshape(-1)\n",
    "        xs = self.embed(x_data)\n",
    "\n",
    "        # GRU\n",
    "        hy, ys = self.gru(None, xs, lengths)\n",
    "        last_vecs = hy[0]\n",
    "        y = self.l_out(last_vecs)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(m):\n",
    "    optimizer = optimizers.Adam(alpha=LR, beta1=BETA_1, beta2=BETA_2, eps=EPS)\n",
    "\n",
    "    optimizer.setup(m)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "Done.\n",
      "Extracting files...\n",
      "Done.\n",
      "Trimming to 30000 max-features\n",
      "Padding to length 150\n",
      "(25000, 150) (25000, 150) (25000,) (25000,)\n",
      "int64 int64 int64 int64\n",
      "CPU times: user 5.45 s, sys: 668 ms, total: 6.12 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Data into format for library\n",
    "x_train, x_test, y_train, y_test = imdb_for_library(seq_len=MAXLEN, max_features=MAXFEATURES)\n",
    "# Torch-specific\n",
    "x_train = x_train.astype(np.int64)\n",
    "x_test = x_test.astype(np.int64)\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 796 ms, sys: 600 ms, total: 1.4 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create symbol\n",
    "sym = SymbolModule(MAXFEATURES, NUMHIDDEN)\n",
    "if GPU:\n",
    "    chainer.cuda.get_device(0).use()  # Make a specified GPU current\n",
    "    sym.to_gpu()  # Copy the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 119 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = init_model(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "CPU times: user 17.6 s, sys: 916 ms, total: 18.5 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cuda.cupy.cuda.profiler.initialize('', 'rnn.nvvp', cuda.cupy.cuda.profiler.cudaKeyValuePair)\n",
    "cuda.cupy.cuda.profiler.start()\n",
    "for j in range(EPOCHS):\n",
    "    for data, target in yield_mb(x_train, y_train, BATCHSIZE, shuffle=True):\n",
    "        # Get samples\n",
    "        data = cuda.to_gpu(data)\n",
    "        target = cuda.to_gpu(target)\n",
    "        output = sym(data)\n",
    "        loss = F.softmax_cross_entropy(output, target)\n",
    "        sym.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "    # Log\n",
    "    print(j)\n",
    "cuda.cupy.cuda.profiler.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 88 ms, total: 1.91 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_samples = (y_test.shape[0]//BATCHSIZE)*BATCHSIZE\n",
    "y_guess = np.zeros(n_samples, dtype=np.int)\n",
    "y_truth = y_test[:n_samples]\n",
    "c = 0\n",
    "\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    for data, target in yield_mb(x_test, y_test, BATCHSIZE):\n",
    "        # Forwards\n",
    "        pred = cuda.to_cpu(sym(cuda.to_gpu(data)).data.argmax(-1))\n",
    "        # Collect results\n",
    "        y_guess[c*BATCHSIZE:(c+1)*BATCHSIZE] = pred\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.837820512821\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", sum(y_guess == y_truth)/len(y_guess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
